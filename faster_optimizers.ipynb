{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faster-optimizers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/11-training-deep-neural-networks/faster_optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ko31BpKKyDt",
        "colab_type": "text"
      },
      "source": [
        "# Faster Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxEbaqdMKy_1",
        "colab_type": "text"
      },
      "source": [
        "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): \n",
        "\n",
        "* applying a good initialization strategy for the connection weights, \n",
        "* using a good activation function, \n",
        "* using Batch Normalization, \n",
        "* and reusing parts of a pretrained network\n",
        "\n",
        "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. Lets analyze these most popular ones: \n",
        "* Momentum optimization, \n",
        "* Nesterov Accelerated Gradient, \n",
        "* AdaGrad, \n",
        "* RMSProp, \n",
        "* and finally Adam and Nadam optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdI92unKWoyB",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM46ucBbWraU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5de3ed05-d6bf-4be9-fb57-c064fb6e594d"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "# %tensorflow_version only exists in Colab.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow.keras.backend as keras_backend\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMLGgnfZW2TG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "3cec3309-57dd-4595-ec8b-b53e83d938c4"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train_full = X_train_full / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoLwH7d5U2tr",
        "colab_type": "text"
      },
      "source": [
        "## Momentum optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZeu4RnEUxLm",
        "colab_type": "text"
      },
      "source": [
        "Recall that Gradient Descent simply updates the weights θ by directly subtracting the gradient of the cost function J(θ) with regards to the weights (∇θJ(θ)) multiplied by the learning rate η.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/gradient.PNG?raw=1' width='800'/>\n",
        "\n",
        "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
        "\n",
        "Momentum optimization cares a great deal about what previous gradients were: at\n",
        "each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by simply adding this momentum vector.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/momentum-algo.PNG?raw=1' width='800'/>\n",
        "\n",
        "In other words, the gradient is used for acceleration,\n",
        "not for speed. To simulate some sort of friction mechanism and prevent the\n",
        "momentum from growing too large, the algorithm introduces a new hyperparameter\n",
        "β, simply called the momentum, which must be set between 0 (high friction) and 1\n",
        "(no friction). A typical momentum value is 0.9.\n",
        "\n",
        "Implementing Momentum optimization in Keras is a no-brainer: just use the SGD\n",
        "optimizer and set its momentum hyperparameter, then lie back and profit!\n",
        "\n",
        "```python\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "```\n",
        "\n",
        "The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAP80sAfWiUJ",
        "colab_type": "text"
      },
      "source": [
        "## Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFTrEPRHWjVg",
        "colab_type": "text"
      },
      "source": [
        "One small variant to Momentum optimization is almost always faster than vanilla Momentum optimization. The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/nesterov-algo.PNG?raw=1' width='800'/>\n",
        "\n",
        "NAG will almost always speed up training compared to regular Momentum optimization. To use it, simply set nesterov=True when creating the SGD optimizer.\n",
        "\n",
        "```python\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
        "```\n",
        "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/nesterov-momentum-optimization.PNG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VM3RYo4c10K",
        "colab_type": "text"
      },
      "source": [
        "## AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oie1CEFpc2xN",
        "colab_type": "text"
      },
      "source": [
        "The AdaGrad algorithm achieves this by scaling down the gradient vector along the steepest dimensions.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/adagrad-algo.PNG?raw=1' width='800'/>\n",
        "\n",
        "* The first step accumulates the square of the gradients into the vector s (recall that the ⊗ symbol represents the element-wise multiplication).If the cost function is steep along the ith dimension, then si will get larger and larger at each iteration.\n",
        "* The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of square root of s + epsilon(the ⊘ symbol represents the element-wise division, and ϵ is a smoothing term to avoid division by zero, typically set to 10–10).\n",
        "\n",
        "This algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum.One additional benefit is that it requires much less tuning of the learning rate hyperparameter η.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/adagrad-versus-gradient-descent.PNG?raw=1' width='800'/>\n",
        "\n",
        "AdaGrad often performs well for simple quadratic problems, but unfortunately it\n",
        "often stops too early when training neural networks. The learning rate gets scaled\n",
        "down so much that the algorithm ends up stopping entirely before reaching the\n",
        "global optimum. So even though Keras has an Adagrad optimizer, you should not use\n",
        "it to train deep neural networks (it may be efficient for simpler tasks such as Linear\n",
        "Regression, though). However, understanding Adagrad is helpful to grasp the other\n",
        "adaptive learning rate optimizers.\n",
        "\n",
        "```python\n",
        "optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VXLlPNedbE",
        "colab_type": "text"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMOibg79e4WP",
        "colab_type": "text"
      },
      "source": [
        "Although AdaGrad slows down a bit too fast and ends up never converging to the\n",
        "global optimum, the RMSProp algorithm15 fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/rmsprop-algo.PNG?raw=1' width='800'/>\n",
        "\n",
        "The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but\n",
        "this default value often works well, so you may not need to tune it at all.\n",
        "\n",
        "```python\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "```\n",
        "\n",
        "Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wePqoVhlW3",
        "colab_type": "text"
      },
      "source": [
        "## Adam and Nadam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kIOg6byhmmS",
        "colab_type": "text"
      },
      "source": [
        "Adam, which stands for adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/adam-algo.PNG?raw=1' width='800'/>\n",
        "\n",
        "If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both Momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 – β1 times the decaying sum).\n",
        "\n",
        "The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scaling decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing term ϵ is usually initialized to a tiny number such as 10–7.\n",
        "\n",
        "```python\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "```\n",
        "\n",
        "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\n",
        "requires less tuning of the learning rate hyperparameter η. You can often use the default value η = 0.001, making Adam even easier to use than Gradient Descent.\n",
        "\n",
        "All the optimization techniques discussed so far only rely on the first-order partial derivatives (Jacobians). The optimization literature contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n2 Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. \n",
        "\n",
        "Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4mjbdCglyIb",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKsJjBiglzOY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}